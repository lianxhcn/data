{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f135a0",
   "metadata": {},
   "source": [
    "### Download transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce4590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from playwright.async_api._generated import Page\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37517a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‘½å\n",
    "def sanitize_filename(title):\n",
    "    \"\"\"\n",
    "    Sanitizes a string to be a valid filename by replacing invalid characters.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e189a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è§£å†³åçˆ¬éªŒè¯æœºåˆ¶\n",
    "async def wait_for_captcha_and_resume(page: Page):\n",
    "    \"\"\"\n",
    "    Checks for a captcha page and pauses the script for manual resolution.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¤– Checking for CAPTCHA...\")\n",
    "    try:\n",
    "        captcha_button = page.get_by_text(\"æŒ‰ä½\").first\n",
    "        await captcha_button.wait_for(state=\"visible\", timeout=5000)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"âš ï¸ CAPTCHA detected! Please manually solve it in the browser.\")\n",
    "        print(\"ğŸ’¡ Once done, return to the console and press Enter to continue.\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        await asyncio.to_thread(input, \"Press Enter to continue...\")\n",
    "        \n",
    "        await page.wait_for_load_state('domcontentloaded')\n",
    "        print(\"âœ… CAPTCHA solved, resuming execution.\")\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"ğŸš€ No CAPTCHA detected, continuing.\")\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cc3544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½å•é¡µtranscript\n",
    "async def download_transcript(page: Page, url: str, output_dir: str, title: str):\n",
    "    \"\"\"\n",
    "    Downloads a transcript using an existing Playwright page.\n",
    "    Returns True if successful, a tuple (False, error_message) otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        \n",
    "        await page.goto(url, timeout=60000)\n",
    "        \n",
    "        await wait_for_captcha_and_resume(page)\n",
    "        \n",
    "        await page.wait_for_load_state(\"networkidle\")\n",
    "\n",
    "        # Use a more specific locator to avoid \"strict mode\" errors\n",
    "        transcript_locator = 'div.T2G6W[data-test-id=\"content-container\"]'\n",
    "        \n",
    "        error_page_check = page.locator('h1[data-test-id=\"yikes-page-title\"]')\n",
    "        if await error_page_check.is_visible():\n",
    "            raise Exception(\"Page not found on Seeking Alpha.\")\n",
    "        \n",
    "        await page.wait_for_selector(transcript_locator, timeout=10000)\n",
    "        \n",
    "        transcript_div = page.locator(transcript_locator)\n",
    "        transcript_text = await transcript_div.inner_text()\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        file_path = os.path.join(output_dir, f\"{title}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(transcript_text)\n",
    "        \n",
    "        print(f\"âœ… Transcript saved to {file_path}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch any exception and return it as a failure\n",
    "        return False, str(e)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸»æµç¨‹\n",
    "async def main():\n",
    "    json_dir = \"/Users/mac/Documents/RA/Olivia_Gu/text_analysis/scraper/7/scraper2.0/æµ‹è¯•çˆ¬å–æ–‡ç« /test_json\"\n",
    "    main_output_dir = \"/Users/mac/Documents/RA/Olivia_Gu/text_analysis/scraper/7/scraper2.0/æµ‹è¯•çˆ¬å–æ–‡ç« /transcript\"\n",
    "    \n",
    "    json_files = glob.glob(os.path.join(json_dir, \"*.json\"))\n",
    "    print(f\"Found {len(json_files)} JSON files to process.\")\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        try:\n",
    "            browser = await p.chromium.connect_over_cdp(\"http://localhost:9222\")\n",
    "            context = browser.contexts[0]\n",
    "            page = context.pages[0]\n",
    "        except Exception as e:\n",
    "            print(f\"â— Failed to connect to browser instance: {e}\")\n",
    "            print(\"Please ensure you have run '.../Google Chrome --remote-debugging-port=9222 ...' in the terminal.\")\n",
    "            return\n",
    "\n",
    "        for file_path in json_files:\n",
    "            try:\n",
    "                if os.stat(file_path).st_size == 0:\n",
    "                    print(f\"â— Skipping empty file: {os.path.basename(file_path)}\")\n",
    "                    continue\n",
    "\n",
    "                file_name = os.path.basename(file_path)\n",
    "                ticker = os.path.splitext(file_name)[0].split(\"_\")[-1]\n",
    "\n",
    "                ticker_output_dir = os.path.join(main_output_dir, ticker)\n",
    "                os.makedirs(ticker_output_dir, exist_ok=True)\n",
    "                \n",
    "                # å®šä¹‰æ¯ä¸ªå…¬å¸çš„å¤±è´¥æ—¥å¿—æ–‡ä»¶è·¯å¾„\n",
    "                failed_links_file = os.path.join(ticker_output_dir, \"failed.json\")\n",
    "                failed_links = []\n",
    "                \n",
    "                downloaded_files = glob.glob(os.path.join(ticker_output_dir, \"*.txt\"))\n",
    "                \n",
    "                print(f\"\\n--- Processing Ticker: {ticker} ---\")\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                results = [(item.get('title'), item.get('link')) for item in data]\n",
    "\n",
    "                for title, link in results:\n",
    "                    if not title or not link:\n",
    "                        continue\n",
    "\n",
    "                    safe_title = sanitize_filename(title)\n",
    "                    \n",
    "                    # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½\n",
    "                    if any(f\"{safe_title}.txt\" == os.path.basename(f) for f in downloaded_files):\n",
    "                        print(f\"âœ… Transcript for '{title}' already exists. Skipping.\")\n",
    "                        continue\n",
    "                    \n",
    "                    clean_url = link.split('#')[0]\n",
    "                    if 'Earnings' in title:\n",
    "                        download_result = await download_transcript(page, clean_url, ticker_output_dir, safe_title)\n",
    "                        \n",
    "                        # æ£€æŸ¥ä¸‹è½½ç»“æœï¼Œå¦‚æœå¤±è´¥åˆ™æ·»åŠ åˆ°åˆ—è¡¨ä¸­\n",
    "                        if isinstance(download_result, tuple):\n",
    "                            _, error_message = download_result\n",
    "                            failed_links.append({\n",
    "                                \"title\": title,\n",
    "                                \"link\": clean_url,\n",
    "                                \"error\": error_message\n",
    "                            })\n",
    "                        \n",
    "                        delay = random.uniform(2, 5)\n",
    "                        print(f\"Pausing for {delay:.2f} seconds...\")\n",
    "                        await asyncio.sleep(delay)\n",
    "                \n",
    "                # å¾ªç¯ç»“æŸåï¼Œå°†è¯¥å…¬å¸çš„å¤±è´¥é“¾æ¥ä¿å­˜åˆ°å…¶ä¸“å±çš„ failed.json æ–‡ä»¶\n",
    "                if failed_links:\n",
    "                    with open(failed_links_file, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(failed_links, f, indent=4, ensure_ascii=False)\n",
    "                    print(f\"â— {len(failed_links)} failed links saved to {failed_links_file}\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"â— Error: {os.path.basename(file_path)} is not a valid JSON file. Skipping.\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"â— An unexpected error occurred while processing {file_path}: {e}\")\n",
    "\n",
    "    print(\"\\n--- All done. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9446056",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
