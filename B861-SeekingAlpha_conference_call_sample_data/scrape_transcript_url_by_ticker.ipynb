{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4382201",
   "metadata": {},
   "source": [
    "### Scrape  transcript url by ticler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# æ¨¡æ‹Ÿæ»šåŠ¨è·å–é“¾æ¥\n",
    "async def scroll_page_until_count_stops(page):\n",
    "    prev_article_count = 0\n",
    "    same_count = 0\n",
    "    max_same = 3 \n",
    "\n",
    "    while True:\n",
    "        await page.mouse.wheel(0, random.randint(1500, 4000))\n",
    "        await asyncio.sleep(random.uniform(0.8, 1.5))\n",
    "\n",
    "        article_links = await page.query_selector_all('h3 a[data-test-id=\"post-list-item-title\"]')\n",
    "        curr_article_count = len(article_links)\n",
    "\n",
    "        if curr_article_count > prev_article_count:\n",
    "            print(f\"âœ… å·²åŠ è½½ {curr_article_count} ä¸ªæ–‡ç« é“¾æ¥...\")\n",
    "            prev_article_count = curr_article_count\n",
    "            same_count = 0\n",
    "        else:\n",
    "            same_count += 1\n",
    "            if same_count >= max_same:\n",
    "                print(\"ğŸ›‘ è¿ç»­å¤šæ¬¡æœªåŠ è½½æ–°é“¾æ¥ï¼Œåœæ­¢æ»šåŠ¨ã€‚\")\n",
    "                break\n",
    "# äººå·¥éªŒè¯\n",
    "async def wait_for_captcha_and_resume(page):\n",
    "    \"\"\"\n",
    "    æ£€æµ‹éªŒè¯ç é¡µé¢ï¼Œå¦‚æœå­˜åœ¨ï¼Œåˆ™æš‚åœç¨‹åºç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨è§£å†³ã€‚\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¤– æ­£åœ¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯ç ...\")\n",
    "    try:\n",
    "        # ä½¿ç”¨æ–‡æœ¬å†…å®¹ç²¾ç¡®å®šä½â€œæŒ‰ä½â€æŒ‰é’®ï¼Œå¹¶è®¾ç½®ä¸€ä¸ªçŸ­è¶…æ—¶\n",
    "        captcha_button = page.get_by_text(\"æŒ‰ä½\").first\n",
    "        await captcha_button.wait_for(state=\"visible\", timeout=5000)\n",
    "        \n",
    "        # å¦‚æœæŒ‰é’®å¯è§ï¼Œè¯´æ˜éªŒè¯ç å·²å‡ºç°\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"âš ï¸ æ£€æµ‹åˆ°éªŒè¯ç ï¼è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®ŒæˆéªŒè¯ã€‚\")\n",
    "        print(\"ğŸ’¡ å®Œæˆåï¼Œè¯·å›åˆ°æ­¤ç»ˆç«¯å¹¶æŒ‰ Enter é”®ç»§ç»­ã€‚\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # æš‚åœç¨‹åºï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥\n",
    "        input(\"æŒ‰ä¸‹ Enter é”®ä»¥ç»§ç»­...\")\n",
    "        \n",
    "        # éªŒè¯ç è§£å†³åï¼Œç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ\n",
    "        await page.wait_for_load_state('domcontentloaded')\n",
    "        print(\"âœ… éªŒè¯ç å·²è§£å†³ï¼Œç¨‹åºç»§ç»­ã€‚\")\n",
    "        \n",
    "    except Exception:\n",
    "        # å¦‚æœåœ¨è¶…æ—¶æ—¶é—´å†…æ²¡æœ‰æ‰¾åˆ°éªŒè¯ç ï¼Œåˆ™è®¤ä¸ºä¸å­˜åœ¨ï¼Œç›´æ¥ç»§ç»­\n",
    "        print(\"ğŸš€ æœªæ£€æµ‹åˆ°éªŒè¯ç ï¼Œç»§ç»­æ‰§è¡Œã€‚\")\n",
    "        pass\n",
    "\n",
    "# è¿›åº¦äºå¼‚å¸¸è®°å½•\n",
    "def get_completed_tickers(progress_file=\"progress_second.txt\"):\n",
    "    \"\"\"ä»è¿›åº¦æ–‡ä»¶ä¸­è¯»å–å·²å®Œæˆçš„ ticker\"\"\"\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, \"r\") as f:\n",
    "            return set(line.strip() for line in f)\n",
    "    return set()\n",
    "\n",
    "def mark_as_completed(ticker, progress_file=\"progress_second.txt\"):\n",
    "    \"\"\"å°†å·²å®Œæˆçš„ ticker å†™å…¥è¿›åº¦æ–‡ä»¶\"\"\"\n",
    "    with open(progress_file, \"a\") as f:\n",
    "        f.write(f\"{ticker}\\n\")\n",
    "\n",
    "def mark_as_excluded(ticker, exclude_file=\"exclude_transcript_second.txt\"):\n",
    "    \"\"\"å°†æ²¡æœ‰ transcripts çš„ ticker å†™å…¥å•ç‹¬æ–‡ä»¶\"\"\"\n",
    "    if os.path.exists(exclude_file):\n",
    "        with open(exclude_file, \"a\") as f:\n",
    "            f.write(f\"{ticker}\\n\")\n",
    "    else:\n",
    "        with open(exclude_file, \"w\") as f:\n",
    "            f.write(f\"{ticker}\\n\")\n",
    "\n",
    "def mark_as_failed(ticker, failed_file=\"failed_second.txt\"):\n",
    "    \"\"\"å°†çˆ¬å–å¤±è´¥çš„ ticker å†™å…¥å•ç‹¬æ–‡ä»¶\"\"\"\n",
    "    if os.path.exists(failed_file):\n",
    "        with open(failed_file, \"a\") as f:\n",
    "            f.write(f\"{ticker}\\n\")\n",
    "    else:\n",
    "        with open(failed_file, \"w\") as f:\n",
    "            f.write(f\"{ticker}\\n\")\n",
    "\n",
    "# ä¸»æµç¨‹\n",
    "async def connect_and_scrape(ticker_list):\n",
    "    print(\"ğŸš€ æ­£åœ¨è¿æ¥ Chrome...\")\n",
    "    async with async_playwright() as p:\n",
    "        try:\n",
    "            browser = await p.chromium.connect_over_cdp(\"http://localhost:9333\")\n",
    "            context = browser.contexts[0]\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è¿æ¥å¤±è´¥: {e}\")\n",
    "            return\n",
    "\n",
    "        completed_tickers = get_completed_tickers()\n",
    "        excluded_tickers = get_completed_tickers(progress_file=\"exclude_transcript_second.txt\")\n",
    "        # æ–°å¢è¯»å–å¤±è´¥çš„ ticker åˆ—è¡¨\n",
    "        failed_tickers = get_completed_tickers(progress_file=\"failed_second.txt\")\n",
    "        \n",
    "        # ç»¼åˆè€ƒè™‘æ‰€æœ‰å·²å¤„ç†çš„ ticker\n",
    "        all_processed_tickers = completed_tickers.union(excluded_tickers).union(failed_tickers)\n",
    "        tickers_to_process = [t for t in ticker_list if t not in all_processed_tickers]\n",
    "\n",
    "        if not tickers_to_process:\n",
    "            print(\"ğŸ‰ æ‰€æœ‰ tickers ä»»åŠ¡å·²å®Œæˆã€å·²æ’é™¤æˆ–å·²å¤±è´¥ï¼Œæ— éœ€å†æ¬¡è¿è¡Œã€‚\")\n",
    "            return\n",
    "\n",
    "        print(f\"ğŸ“„ å°†ä» {len(tickers_to_process)} ä¸ªæœªå®Œæˆçš„ tickers å¼€å§‹çˆ¬å–...\")\n",
    "\n",
    "        output_folder = \"transcripts_second\"\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "            print(f\"ğŸ“ å·²åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹: {output_folder}\")\n",
    "        \n",
    "        # ç¡®ä¿æ’é™¤å’Œå¤±è´¥æ–‡ä»¶å­˜åœ¨\n",
    "        exclude_file = \"exclude_transcript_second.txt\"\n",
    "        if not os.path.exists(exclude_file):\n",
    "            with open(exclude_file, \"w\") as f:\n",
    "                pass\n",
    "        failed_file = \"failed_second.txt\"\n",
    "        if not os.path.exists(failed_file):\n",
    "            with open(failed_file, \"w\") as f:\n",
    "                pass\n",
    "\n",
    "        for ticker in tickers_to_process:\n",
    "            url = f'https://seekingalpha.com/symbol/{ticker}/earnings/transcripts'\n",
    "            \n",
    "            try:\n",
    "                page = None\n",
    "                pages = context.pages\n",
    "                for existing_page in pages:\n",
    "                    if \"seekingalpha.com\" in existing_page.url:\n",
    "                        page = existing_page\n",
    "                        break\n",
    "                \n",
    "                if not page:\n",
    "                    page = await context.new_page()\n",
    "\n",
    "                print(f\"\\nâ¡ï¸ æ­£åœ¨å¤„ç† {ticker}...\")\n",
    "                print(f\"â¡ï¸ å¯¼èˆªåˆ° {url}\")\n",
    "                await page.goto(url, timeout=60000)\n",
    "                \n",
    "                await wait_for_captcha_and_resume(page)\n",
    "\n",
    "                print(\"ğŸ” æ£€æŸ¥ 'Transcripts & Insights' æŒ‰é’®æ˜¯å¦å­˜åœ¨...\")\n",
    "                transcripts_button = await page.query_selector('a[data-test-id=\"earnings/transcripts\"]')\n",
    "                \n",
    "                if not transcripts_button:\n",
    "                    print(f\"âš ï¸ {ticker} é¡µé¢ä¸åŒ…å« 'Transcripts & Insights' æŒ‰é’®ã€‚\")\n",
    "                    print(f\"ğŸ“„ ä¸º {ticker} åˆ›å»ºä¸€ä¸ªç©ºçš„ JSON æ–‡ä»¶å¹¶æ ‡è®°ä¸ºå·²æ’é™¤ã€‚\")\n",
    "                    \n",
    "                    results = []\n",
    "                    file_path = os.path.join(output_folder, f\"{ticker}.json\")\n",
    "                    with open(file_path, \"w\", encoding='utf-8') as f:\n",
    "                        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "                    \n",
    "                    mark_as_excluded(ticker)\n",
    "                    print(f\"âœ… {ticker} æ•°æ®å·²æˆåŠŸä¿å­˜è‡³ {file_path} (æ— å†…å®¹)ã€‚\")\n",
    "                    continue\n",
    "                \n",
    "                print(\"âœ… æŒ‰é’®å­˜åœ¨ï¼Œå¼€å§‹çˆ¬å–ã€‚\")\n",
    "\n",
    "                print(\"âŒ› ç­‰å¾…é¡µé¢åŠ è½½æ–‡ç« å†…å®¹...\")\n",
    "                await page.wait_for_selector('h3 a[data-test-id=\"post-list-item-title\"]', timeout=30000)\n",
    "                print(\"âœ… é¡µé¢åŠ è½½å®Œæ¯•ã€‚\")\n",
    "\n",
    "                print(\"ğŸš€ å¼€å§‹è‡ªåŠ¨æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹...\")\n",
    "                await scroll_page_until_count_stops(page)\n",
    "\n",
    "                article_links = await page.query_selector_all('h3 a[data-test-id=\"post-list-item-title\"]')\n",
    "                print(f\"ğŸ“„ å…±æ‰¾åˆ° {len(article_links)} ä¸ªæ–‡ç« é“¾æ¥ for {ticker}\")\n",
    "\n",
    "                results = []\n",
    "                for link_elem in article_links:\n",
    "                    href = await link_elem.get_attribute('href')\n",
    "                    title = await link_elem.inner_text()\n",
    "                    \n",
    "                    if href:\n",
    "                        clean_href = href.split('#')[0]\n",
    "                        results.append({\n",
    "                            \"title\": title.strip(),\n",
    "                            \"link\": f\"https://seekingalpha.com{clean_href}\"\n",
    "                        })\n",
    "\n",
    "                print(f\"ğŸ“„ æœ€ç»ˆå¤„ç†åæ‰¾åˆ° {len(results)} ä¸ªé“¾æ¥\")\n",
    "                \n",
    "                file_path = os.path.join(output_folder, f\"{ticker}.json\")\n",
    "                with open(file_path, \"w\", encoding='utf-8') as f:\n",
    "                    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "                mark_as_completed(ticker)\n",
    "                print(f\"âœ… {ticker} æ•°æ®å·²æˆåŠŸä¿å­˜è‡³ {file_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ æŠ“å– {ticker} å¤±è´¥: {e}\")\n",
    "                print(f\"âš ï¸ é”™è¯¯ä¿¡æ¯: {e}\")\n",
    "                mark_as_failed(ticker)\n",
    "                \n",
    "        await browser.close()\n",
    "        print(\"âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œæµè§ˆå™¨å·²å…³é—­ã€‚\")\n",
    "\n",
    "# å‡è®¾ä½ å·²æœ‰ä¸€ä¸ª ticker_listï¼Œä¾‹å¦‚ï¼š\n",
    "# ticker_list = [\"AAPL\", \"MSFT\", \"NVDA\", ...]\n",
    "# å®é™…ä¸Šè¿™é‡Œçš„ticker_listå·²ç»æ”¾åœ¨ github ä»“åº“ä¸­ï¼Œæ˜¯SeekingAlphaæˆªæ­¢2025å¹´9æœˆä»½æ‰€æœ‰å¯è·å–çš„ä¼ä¸šä»£ç çš„codes.jsonæ–‡ä»¶\n",
    "# éœ€è¦å°†codes.jsonæ–‡ä»¶å¯¼å…¥åè¿è¡Œ\n",
    "asyncio.run(connect_and_scrape(ticker_list))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
